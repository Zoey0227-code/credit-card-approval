{"cells":[{"source":"\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\ncc_apps.info()\ncc_apps.describe()\ncc_apps.head()\n\n(cc_apps == '?').sum() # Check how many \"?\" are in the data set\ncc_clean = cc_apps.replace(\"?\",np.nan) #Replace with np.nan so pandas can detect missing values\ncc_clean.info() #double check if it worked (should see non-null number dropped)\ndf = cc_clean.copy() #work on a new copy (justincase)\n# Inspect unique values in column 0\nprint(cc_clean[0].unique())\n\n# Or for every column, see the first few uniques:\nfor col in cc_clean.columns:\n    print(col, cc_clean[col].unique()[:10])\n\nfor col in df.columns:\n    if df[col].dtype =='object':\n        mode_val = df[col].value_counts().idxmax()\n        df[col].fillna(mode_val,inplace=True)\n    else:\n        mean_val = df[col].mean()\n        df[col].fillna(mean_val,inplace = True)\n\ndf_encoded = pd.get_dummies(df, drop_first = True)\nx = df_encoded.iloc[:, :-1].values\ny = df_encoded.iloc[:,-1].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    x, y,\n    test_size = 0.3,\n    random_state = 42\n    )\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n#train and evaluate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix #import the needed \nmodel = LogisticRegression() #start with default\nmodel.fit(X_train_scaled, y_train) #adjust the weighs to minimize errors/learning phase\ny_pred = model.predict(X_test_scaled) #examination (testing) phase, after this step, we get y_pred for the next step to compare to the ACTUAL result\nconfusion_matrix(y_test, y_pred) #test is the 30, not 70 / TP,TN,FP,FN\nmodel.score(X_test_scaled, y_test)\n\n#compare hyperparameters\nCs = [0.01, 0.1, 1, 10, 100] #regularizations strengths to try\npenalties = ['l1', 'l2'] # penalty types\nsolvers = ['liblinear']\nparam_grid = {\n    \"C\"      : Cs,\n    \"penalty\": penalties,\n    \"solver\" : solvers\n}\n\n#setting up and running grid search \nfrom sklearn.model_selection import GridSearchCV #try all hyperparameter combos + cross-validate\nbase_model = LogisticRegression(max_iter = 1000) #max_iter to makesure solver have enough steps to solve\ngrid = GridSearchCV(\n    estimator = base_model, #which model to tune\n    param_grid = param_grid, #that's the dictionary of hyperparameter choices\n    cv = 5, #requests 5-fold cross-validation \n    scoring = \"accuracy\" # compare models by their average accuracy\n)\ngrid.fit(X_train_scaled, y_train)\n\n#inspecting the best hyperparameters\nprint(\"Best parameters: \", grid.best_params_)\nprint(\"CV best score: \", grid.best_score_)\nprint(\"Test accuracy: \", grid.score(X_test_scaled, y_test))\n\n#grab the best model\nbest_model = grid.best_estimator_\n\n#make final predictions\ny_pred_best = best_model.predict(X_test_scaled)\n\n#compute and display the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix (y_test, y_pred_best)\nprint(\"Confusion matrix:\\n\", cm)\nbest_score = best_model.score(X_test_scaled, y_test)\nprint(\"Final test-set accuracy:\", round(best_score, 3))\n","metadata":{"executionCancelledAt":null,"executionTime":11362,"lastExecutedAt":1754642859168,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\ncc_apps.info()\ncc_apps.describe()\ncc_apps.head()\n\n(cc_apps == '?').sum() # Check how many \"?\" are in the data set\ncc_clean = cc_apps.replace(\"?\",np.nan) #Replace with np.nan so pandas can detect missing values\ncc_clean.info() #double check if it worked (should see non-null number dropped)\ndf = cc_clean.copy() #work on a new copy (justincase)\n# Inspect unique values in column 0\nprint(cc_clean[0].unique())\n\n# Or for every column, see the first few uniques:\nfor col in cc_clean.columns:\n    print(col, cc_clean[col].unique()[:10])\n\nfor col in df.columns:\n    if df[col].dtype =='object':\n        mode_val = df[col].value_counts().idxmax()\n        df[col].fillna(mode_val,inplace=True)\n    else:\n        mean_val = df[col].mean()\n        df[col].fillna(mean_val,inplace = True)\n\ndf_encoded = pd.get_dummies(df, drop_first = True)\nx = df_encoded.iloc[:, :-1].values\ny = df_encoded.iloc[:,-1].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    x, y,\n    test_size = 0.3,\n    random_state = 42\n    )\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n#train and evaluate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix #import the needed \nmodel = LogisticRegression() #start with default\nmodel.fit(X_train_scaled, y_train) #adjust the weighs to minimize errors/learning phase\ny_pred = model.predict(X_test_scaled) #examination (testing) phase, after this step, we get y_pred for the next step to compare to the ACTUAL result\nconfusion_matrix(y_test, y_pred) #test is the 30, not 70 / TP,TN,FP,FN\nmodel.score(X_test_scaled, y_test)\n\n#compare hyperparameters\nCs = [0.01, 0.1, 1, 10, 100] #regularizations strengths to try\npenalties = ['l1', 'l2'] # penalty types\nsolvers = ['liblinear']\nparam_grid = {\n    \"C\"      : Cs,\n    \"penalty\": penalties,\n    \"solver\" : solvers\n}\n\n#setting up and running grid search \nfrom sklearn.model_selection import GridSearchCV #try all hyperparameter combos + cross-validate\nbase_model = LogisticRegression(max_iter = 1000) #max_iter to makesure solver have enough steps to solve\ngrid = GridSearchCV(\n    estimator = base_model, #which model to tune\n    param_grid = param_grid, #that's the dictionary of hyperparameter choices\n    cv = 5, #requests 5-fold cross-validation \n    scoring = \"accuracy\" # compare models by their average accuracy\n)\ngrid.fit(X_train_scaled, y_train)\n\n#inspecting the best hyperparameters\nprint(\"Best parameters: \", grid.best_params_)\nprint(\"CV best score: \", grid.best_score_)\nprint(\"Test accuracy: \", grid.score(X_test_scaled, y_test))\n\n#grab the best model\nbest_model = grid.best_estimator_\n\n#make final predictions\ny_pred_best = best_model.predict(X_test_scaled)\n\n#compute and display the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix (y_test, y_pred_best)\nprint(\"Confusion matrix:\\n\", cm)\nbest_score = best_model.score(X_test_scaled, y_test)\nprint(\"Final test-set accuracy:\", round(best_score, 3))\n","outputsMetadata":{"0":{"height":600,"type":"stream"},"1":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"8325c3a7-24f4-4f28-ac50-110cd5de2658","nodeType":"const"}}}},"lastExecutedByKernel":"0e65e1d4-11b2-4e1f-82e0-7f4efe6b44ca","visualizeDataframe":false,"version":"ag-charts-v1"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       678 non-null    object \n 1   1       678 non-null    object \n 2   2       690 non-null    float64\n 3   3       684 non-null    object \n 4   4       684 non-null    object \n 5   5       681 non-null    object \n 6   6       681 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\n['b' 'a' nan]\n0 ['b' 'a' nan]\n1 ['30.83' '58.67' '24.50' '27.83' '20.17' '32.08' '33.17' '22.92' '54.42'\n '42.50']\n2 [ 0.     4.46   0.5    1.54   5.625  4.     1.04  11.585  4.915  0.83 ]\n3 ['u' 'y' nan 'l']\n4 ['g' 'p' nan 'gg']\n5 ['w' 'q' 'm' 'r' 'cc' 'k' 'c' 'd' 'x' 'i']\n6 ['v' 'h' 'bb' 'ff' 'j' 'z' nan 'o' 'dd' 'n']\n7 [1.25  3.04  1.5   3.75  1.71  2.5   6.5   0.04  3.96  3.165]\n8 ['t' 'f']\n9 ['t' 'f']\n10 [ 1  6  0  5  7 10  3 17  2  9]\n11 ['g' 's' 'p']\n12 [    0   560   824     3 31285  1349   314  1442   200  2690]\n13 ['+' '-']\nBest parameters:  {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\nCV best score:  0.861168384879725\nTest accuracy:  0.8405797101449275\nConfusion matrix:\n [[89  8]\n [25 85]]\nFinal test-set accuracy: 0.841\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}